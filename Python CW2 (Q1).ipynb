{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **CMT309 - Computational Data Science - Data Science Portfolio**"
      ],
      "metadata": {
        "id": "t_hYgOgUzIwe"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCAPWR2s3nzA"
      },
      "source": [
        "# Part 1 - Text Data Analysis (45 marks)\n",
        "\n",
        "In this question you will write Python code for processing, analyzing and understanding the social network **Reddit** (www.reddit.com). Reddit is a platform that allows users to upload posts and comment on them, and is divided in _subreddits_, often covering specific themes or areas of interest (for example, [world news](https://www.reddit.com/r/worldnews/), [ukpolitics](https://www.reddit.com/r/ukpolitics/) or [nintendo](https://www.reddit.com/r/nintendo)). You are provided with a subset of Reddit with posts from Covid-related subreddits (e.g., _CoronavirusUK_ or _NoNewNormal_), as well as randomly selected subreddits (e.g., _donaldtrump_ or _razer_).\n",
        "\n",
        "The `csv` dataset you are provided contains one row per post, and has information about three entities: **posts**, **users** and **subreddits**. The column names are self-explanatory: columns starting with the prefix `user_` describe users, those starting with the prefix `subr_` describe subreddits, the `subreddit` column is the subreddit name, and the rest of the columns are post attributes (`author`, `posted_at`, `title` and post text - the `selftext` column-, number of comments - `num_comments`, `score`, etc.).\n",
        "\n",
        "In this exercise, you are asked to perform a number of operations to gain insights from the data."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## P1.0) Suggested/Required Imports"
      ],
      "metadata": {
        "id": "jlssd-CQOOXE"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Pm74v1u4d6G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "796955d7-0d08-4178-b8a6-90a3f1445e9a"
      },
      "source": [
        "# suggested imports\n",
        "import pandas as pd\n",
        "from nltk.tag import pos_tag\n",
        "import re\n",
        "from collections import defaultdict,Counter\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import os\n",
        "tqdm.pandas()\n",
        "from ast import literal_eval\n",
        "# nltk imports, note that these outputs may be different if you are using colab or local jupyter notebooks\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfNsDQ253nzJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "350c6340-d4de-44c2-9c77-383e7cd872ee"
      },
      "source": [
        "from urllib import request\n",
        "import pandas as pd\n",
        "module_url = f\"https://raw.githubusercontent.com/luisespinosaanke/cmt309-portfolio/master/data_portfolio_22.csv\"\n",
        "module_name = module_url.split('/')[-1]\n",
        "print(f'Fetching {module_url}')\n",
        "#with open(\"file_1.txt\") as f1, open(\"file_2.txt\") as f2\n",
        "with request.urlopen(module_url) as f, open(module_name,'w') as outf:\n",
        "  a = f.read()\n",
        "  outf.write(a.decode('utf-8'))\n",
        "df = pd.read_csv('data_portfolio_22.csv')\n",
        "# this fills empty cells with empty strings\n",
        "df = df.fillna('')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching https://raw.githubusercontent.com/luisespinosaanke/cmt309-portfolio/master/data_portfolio_22.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "CNfbxg2X3nzK",
        "outputId": "5960a1cf-fa69-426d-a1a6-620611f6c747"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       author            posted_at  num_comments  score selftext  \\\n",
              "0  -Howitzer-  2020-08-17 20:26:04            19      1            \n",
              "1  -Howitzer-  2020-07-06 17:01:48             1      3            \n",
              "2  -Howitzer-  2020-09-09 02:29:02             3      1            \n",
              "3  -Howitzer-  2020-06-23 23:02:39             2      1            \n",
              "4  -Howitzer-  2020-08-07 04:13:53            32    622            \n",
              "\n",
              "  subr_created_at              subr_description  \\\n",
              "0      2009-04-29  Subreddit about Donald Trump   \n",
              "1      2009-04-29  Subreddit about Donald Trump   \n",
              "2      2009-04-29  Subreddit about Donald Trump   \n",
              "3      2009-04-29  Subreddit about Donald Trump   \n",
              "4      2009-04-29  Subreddit about Donald Trump   \n",
              "\n",
              "                                       subr_faved_by  subr_numb_members  \\\n",
              "0  ['vergil_never_cry', 'Jelegend', 'pianoyeah', ...              30053   \n",
              "1  ['vergil_never_cry', 'Jelegend', 'pianoyeah', ...              30053   \n",
              "2  ['vergil_never_cry', 'Jelegend', 'pianoyeah', ...              30053   \n",
              "3  ['vergil_never_cry', 'Jelegend', 'pianoyeah', ...              30053   \n",
              "4  ['vergil_never_cry', 'Jelegend', 'pianoyeah', ...              30053   \n",
              "\n",
              "   subr_numb_posts    subreddit  \\\n",
              "0           796986  donaldtrump   \n",
              "1           796986  donaldtrump   \n",
              "2           796986  donaldtrump   \n",
              "3           796986  donaldtrump   \n",
              "4           796986  donaldtrump   \n",
              "\n",
              "                                               title  total_awards_received  \\\n",
              "0  BREAKING: Trump to begin hiding in mailboxes t...                      0   \n",
              "1                                Joe Biden's America                      0   \n",
              "2  4 more years and we can erase his legacy for g...                      0   \n",
              "3  Revelation 9:6 [Transhumanism: The New Religio...                      0   \n",
              "4                                     LOOK HERE, FAT                      0   \n",
              "\n",
              "   upvote_ratio  user_num_posts user_registered_at  user_upvote_ratio  \n",
              "0          1.00            4661         2012-11-09          -0.658599  \n",
              "1          0.67            4661         2012-11-09          -0.658599  \n",
              "2          1.00            4661         2012-11-09          -0.658599  \n",
              "3          1.00            4661         2012-11-09          -0.658599  \n",
              "4          0.88            4661         2012-11-09          -0.658599  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-86c2f17d-2cdb-4ada-ae31-b15e24017280\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>author</th>\n",
              "      <th>posted_at</th>\n",
              "      <th>num_comments</th>\n",
              "      <th>score</th>\n",
              "      <th>selftext</th>\n",
              "      <th>subr_created_at</th>\n",
              "      <th>subr_description</th>\n",
              "      <th>subr_faved_by</th>\n",
              "      <th>subr_numb_members</th>\n",
              "      <th>subr_numb_posts</th>\n",
              "      <th>subreddit</th>\n",
              "      <th>title</th>\n",
              "      <th>total_awards_received</th>\n",
              "      <th>upvote_ratio</th>\n",
              "      <th>user_num_posts</th>\n",
              "      <th>user_registered_at</th>\n",
              "      <th>user_upvote_ratio</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-Howitzer-</td>\n",
              "      <td>2020-08-17 20:26:04</td>\n",
              "      <td>19</td>\n",
              "      <td>1</td>\n",
              "      <td></td>\n",
              "      <td>2009-04-29</td>\n",
              "      <td>Subreddit about Donald Trump</td>\n",
              "      <td>['vergil_never_cry', 'Jelegend', 'pianoyeah', ...</td>\n",
              "      <td>30053</td>\n",
              "      <td>796986</td>\n",
              "      <td>donaldtrump</td>\n",
              "      <td>BREAKING: Trump to begin hiding in mailboxes t...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>4661</td>\n",
              "      <td>2012-11-09</td>\n",
              "      <td>-0.658599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-Howitzer-</td>\n",
              "      <td>2020-07-06 17:01:48</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td></td>\n",
              "      <td>2009-04-29</td>\n",
              "      <td>Subreddit about Donald Trump</td>\n",
              "      <td>['vergil_never_cry', 'Jelegend', 'pianoyeah', ...</td>\n",
              "      <td>30053</td>\n",
              "      <td>796986</td>\n",
              "      <td>donaldtrump</td>\n",
              "      <td>Joe Biden's America</td>\n",
              "      <td>0</td>\n",
              "      <td>0.67</td>\n",
              "      <td>4661</td>\n",
              "      <td>2012-11-09</td>\n",
              "      <td>-0.658599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-Howitzer-</td>\n",
              "      <td>2020-09-09 02:29:02</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td></td>\n",
              "      <td>2009-04-29</td>\n",
              "      <td>Subreddit about Donald Trump</td>\n",
              "      <td>['vergil_never_cry', 'Jelegend', 'pianoyeah', ...</td>\n",
              "      <td>30053</td>\n",
              "      <td>796986</td>\n",
              "      <td>donaldtrump</td>\n",
              "      <td>4 more years and we can erase his legacy for g...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>4661</td>\n",
              "      <td>2012-11-09</td>\n",
              "      <td>-0.658599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-Howitzer-</td>\n",
              "      <td>2020-06-23 23:02:39</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td></td>\n",
              "      <td>2009-04-29</td>\n",
              "      <td>Subreddit about Donald Trump</td>\n",
              "      <td>['vergil_never_cry', 'Jelegend', 'pianoyeah', ...</td>\n",
              "      <td>30053</td>\n",
              "      <td>796986</td>\n",
              "      <td>donaldtrump</td>\n",
              "      <td>Revelation 9:6 [Transhumanism: The New Religio...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>4661</td>\n",
              "      <td>2012-11-09</td>\n",
              "      <td>-0.658599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-Howitzer-</td>\n",
              "      <td>2020-08-07 04:13:53</td>\n",
              "      <td>32</td>\n",
              "      <td>622</td>\n",
              "      <td></td>\n",
              "      <td>2009-04-29</td>\n",
              "      <td>Subreddit about Donald Trump</td>\n",
              "      <td>['vergil_never_cry', 'Jelegend', 'pianoyeah', ...</td>\n",
              "      <td>30053</td>\n",
              "      <td>796986</td>\n",
              "      <td>donaldtrump</td>\n",
              "      <td>LOOK HERE, FAT</td>\n",
              "      <td>0</td>\n",
              "      <td>0.88</td>\n",
              "      <td>4661</td>\n",
              "      <td>2012-11-09</td>\n",
              "      <td>-0.658599</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-86c2f17d-2cdb-4ada-ae31-b15e24017280')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-86c2f17d-2cdb-4ada-ae31-b15e24017280 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-86c2f17d-2cdb-4ada-ae31-b15e24017280');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyQyR27z48nr"
      },
      "source": [
        "## P1.1 - Text data processing (20 marks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfpA4Z5ADdok"
      },
      "source": [
        "### P1.1.1 - Offensive authors per subreddit (5 marks)\n",
        "\n",
        "As you will see, the dataset contains a lot of strings of the form `[***]`. These have been used to mask (or remove) swearwords to make it less offensive. We are interested in finding those users that have posted at least one swearword in each subreddit. We do this by counting occurrences of the `[***]` string in the `selftext` column (we can assume that an occurrence of `[***]` equals a swearword in the original dataset).\n",
        "\n",
        "**What to implement:** A function `offensive_authors(df)` that takes as input the original dataframe and returns a dataframe of the form below, where each row contains authors that posted at least one swearword in the corresponding subreddit.\n",
        "\n",
        "```\n",
        "subreddit\tauthor\n",
        "0\t40kLore\tCross_Ange\n",
        "1\t40kLore\tDaRandomGitty2\n",
        "2\t40kLore\tEMB1981\n",
        "3\t40kLore\tEvoxrus_XV\n",
        "4\t40kLore\tGrtrshop\n",
        "...\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def offensive_authors(df):\n",
        "    # Group the dataframe by subreddit and author, and count the number of occurrences of \"[***]\"\n",
        "    counts = df[df[\"selftext\"].str.contains(\"\\[\\*\\*\\*\\]\")].groupby([\"subreddit\", \"author\"]).size().reset_index(name=\"count\")\n",
        "    \n",
        "    # Get the list of unique subreddits\n",
        "    subreddits = df[\"subreddit\"].unique()\n",
        "    \n",
        "    # Create an empty list to store the results\n",
        "    result = []\n",
        "    \n",
        "    # Iterate over the subreddits\n",
        "    for subreddit in subreddits:\n",
        "        # Get the list of authors that posted in the current subreddit\n",
        "        authors = df[df[\"subreddit\"] == subreddit][\"author\"].unique()\n",
        "        \n",
        "        # Filter the counts dataframe to include only the current subreddit and the authors that posted in it\n",
        "        subreddit_counts = counts[counts[\"subreddit\"] == subreddit]\n",
        "        subreddit_counts = subreddit_counts[subreddit_counts[\"author\"].isin(authors)]\n",
        "        \n",
        "        # Check if all the authors posted at least one swearword in the current subreddit\n",
        "        if (subreddit_counts[\"count\"] > 0).all():\n",
        "            # Append the authors to the result list\n",
        "            result.append(pd.DataFrame({\"subreddit\": [subreddit] * len(authors), \"author\": authors}))\n",
        "    \n",
        "    # Concatenate the dataframes in the result list and return the concatenated dataframe\n",
        "    result = pd.concat(result, ignore_index=True)\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "9oby6Xbz59F0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "offensive_authors(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "RNkjmPH3gESg",
        "outputId": "57d3cbe0-98aa-4d5a-9092-882bbf2ae253"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              subreddit            author\n",
              "0           donaldtrump        -Howitzer-\n",
              "1           donaldtrump      -mylankovic-\n",
              "2           donaldtrump       0ldManFrank\n",
              "3           donaldtrump            11_gop\n",
              "4           donaldtrump    AddictedReddit\n",
              "...                 ...               ...\n",
              "2792  TheVampireDiaries            eli454\n",
              "2793  TheVampireDiaries            kay278\n",
              "2794  TheVampireDiaries  maverickbluezero\n",
              "2795      criminalminds            eli454\n",
              "2796               sony        rkunreal93\n",
              "\n",
              "[2797 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1599d5a2-c072-42de-8a89-129b9dc7707f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>subreddit</th>\n",
              "      <th>author</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>donaldtrump</td>\n",
              "      <td>-Howitzer-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>donaldtrump</td>\n",
              "      <td>-mylankovic-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>donaldtrump</td>\n",
              "      <td>0ldManFrank</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>donaldtrump</td>\n",
              "      <td>11_gop</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>donaldtrump</td>\n",
              "      <td>AddictedReddit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2792</th>\n",
              "      <td>TheVampireDiaries</td>\n",
              "      <td>eli454</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2793</th>\n",
              "      <td>TheVampireDiaries</td>\n",
              "      <td>kay278</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2794</th>\n",
              "      <td>TheVampireDiaries</td>\n",
              "      <td>maverickbluezero</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2795</th>\n",
              "      <td>criminalminds</td>\n",
              "      <td>eli454</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2796</th>\n",
              "      <td>sony</td>\n",
              "      <td>rkunreal93</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2797 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1599d5a2-c072-42de-8a89-129b9dc7707f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1599d5a2-c072-42de-8a89-129b9dc7707f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1599d5a2-c072-42de-8a89-129b9dc7707f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhZ3u5aS3rrm"
      },
      "source": [
        "### P1.1.2 - Most common trigrams per subreddit (15 marks)\n",
        "\n",
        "We are interested in learning about _the ten most frequent trigrams_ (a [trigram](https://en.wikipedia.org/wiki/Trigram) is a sequence of three consecutive words) in each subreddit's content. You must compute these trigrams on both the `selftext` and `title` columns. Your task is to generate a Python dictionary of the form:\n",
        "\n",
        "```\n",
        "{subreddit1: [(trigram1, freq1), (trigram2, freq2), ... , (trigram3, freq10)],\n",
        "subreddit1: [(trigram1, freq1), (trigram2, freq2), ... , (trigram3, freq10)],\n",
        "...\n",
        "subreddit63: [(trigram1, freq1), (trigram2, freq2), ... , (trigram3, freq10)],}\n",
        "```\n",
        "\n",
        "That is, for each subreddit, the 10 most frequent trigrams and their frequency, stored in a list of tuples. Each trigram will be stored also as a tuple containing 3 strings.\n",
        "\n",
        "**What to implement**: A function `get_tris(df, stopwords_list, punctuation_list)` that will take as input the original dataframe, a list of stopwords and a list of punctuation signs (e.g., `?` or `!`), and will return a python dictionary with the above format. Your function must implement the following steps in order:\n",
        "\n",
        "- (**1 mark**) Create a new dataframe called `newdf` with only `subreddit`, `title` and `selftext` columns.\n",
        "- (**1 mark**) Add a new column to `newdf` called `full_text`, which will contain `title` and `selftext` concatenated with the string `.` (a full stop) followed by a space. That, is `A simple title` and `This is a text body` would be `A simple title. This is a text body`.\n",
        "- (**1 mark**) Remove all occurrences of the following strings from `full_text`. You must do this without creating a new column:\n",
        "  - `[***]`\n",
        "  - `&amp;`\n",
        "  - `&gt;`\n",
        "  - `https`\n",
        "- (**1 mark**) You must also remove all occurrences of at least three consecutive hyphens, for example, you should remove strings like `---`, `----`, `-----`, etc., but not `--` and not `-`.\n",
        "- (**1 mark**) Tokenize the contents of the `full_text` column after lower casing (removing all capitalization). You should use the `word_tokenize` function in `nltk`. Add the results to a new column called `full_text_tokenized`.\n",
        "- (**2 mark**) Remove all tokens that are either stopwords or punctuation from `full_text_tokenized` and store the results in a new column called `full_text_tokenized_clean`. _See Note 1_.\n",
        "- (**2 marks**) Create a new dataframe called `adf` (which will stand for _aggregated dataframe_), which will have one row per subreddit (i.e., 63 rows), and will have two columns: `subreddit` (the subreddit name), and `all_words`, which will be a big list with all the words that belong to that subreddit as extracted from the `full_text_tokenized_clean`.\n",
        "- (**3 marks**) Obtain trigram counts, which will be stored in a dictionary where each `key` will be a trigram (a `tuple` containing 3 consecutive tokens), and each `value` will be their overall frequency in that subreddit. You are  encouraged to use functions from the `nltk` package, although you can choose any approach to solve this part.\n",
        "- (**3 marks**) Finally, use the information you have in `adf` for generating the desired dictionary, and return it. _See Note 2_.\n",
        "\n",
        "Note 1. You can obtain stopwords and punctuation as follows.\n",
        "- Stopwords: \n",
        "```\n",
        "from nltk.corpus import stopwords\n",
        "stopwords = stopwords.words('english')\n",
        "```\n",
        "- Punctuation:\n",
        "```\n",
        "import string\n",
        "punctuation = list(string.punctuation)\n",
        "```\n",
        "\n",
        "Note 2. You do not have to apply an additional ordering when there are several trigrams with the same frequency."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# necessary imports here for extra clarity\n",
        "from nltk.corpus import stopwords as sw\n",
        "import string\n",
        "import warnings\n",
        "\n",
        "def get_tris(df, stopwords_list, punctuation_list):\n",
        "   \n",
        "    # 1 MARK - create new df with only relevant columns\n",
        "    newdf = df[['subreddit', 'title', 'selftext']].copy()\n",
        "\n",
        "    # 1 MARK - concatenate title and selftext\n",
        "    newdf['full_text'] = newdf['title'] + '. ' + newdf['selftext']\n",
        "\n",
        "    # 1 MARK for string replacement\n",
        "    newdf['full_text'] = newdf['full_text'].str.replace('\\n', ' ')\n",
        "\n",
        "    # 1 MARK for regex replacement - remove the strings \"[***]\", \"&amp;\", \"&gt;\" and \"https\", also at least three consecutive dashes\n",
        "    newdf['full_text'] = newdf['full_text'].str.replace('\\[.*?\\]|\\&\\w+;|\\&\\#?\\w+;|https?|[-]{3,}', '', regex=True)\n",
        "\n",
        "    # 1 MARK - lower case, tokenize, and add result to full_text_tokenize\n",
        "    newdf['full_text_tokenize'] = newdf['full_text'].str.lower().apply(word_tokenize)\n",
        "\n",
        "    # 2 MARKS - clean the full_text_tokenized column by iterating over each word and discarding if it's either a stopword or punctuation\n",
        "    newdf['full_text_tokenized_clean'] = newdf['full_text_tokenize'].apply(lambda x: [word for word in x if word not in stopwords_list and word not in punctuation_list])\n",
        "\n",
        "    # 2 MARKS - create new aggregated dataframe by concatenating all full_text_tokenized_clean values - rename columns as requested\n",
        "    adf = newdf.groupby('subreddit')['full_text_tokenized_clean'].agg(lambda x: [item for sublist in x for item in sublist]).reset_index()\n",
        "    adf.columns = ['subreddit', 'all_words']\n",
        "\n",
        "    # 3 MARKS - create new Series object by piping nltk's FreqDist and trigrams functions into all_words\n",
        "    tri_counts = adf['all_words'].apply(lambda x: list(nltk.FreqDist(nltk.trigrams(x)).items()))\n",
        "\n",
        "    # 3 MARKS - create output dictionary by zipping subreddit column from adf and tri_counts into a list of tuples, then passing dict()\n",
        "    # the top 10 most frequent ngrams are obtained by calling sorted() on tri_counts and keeping only the top 10 elements\n",
        "    output_dict = dict(zip(adf['subreddit'], tri_counts.apply(lambda x: sorted(x, key=lambda y: y[1], reverse=True)[:10])))\n",
        "    \n",
        "    return output_dict\n"
      ],
      "metadata": {
        "id": "A0NeN7uGftfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #get stopwords as list\n",
        "sw = sw.words('english')\n",
        "# get punctuation as list\n",
        "p = list(string.punctuation)\n",
        "# optional lines for adding the below line to avoid the SettingWithCopyWarning\n",
        "warnings.filterwarnings('ignore')\n",
        "get_tris(df, sw, p)\n",
        "\n"
      ],
      "metadata": {
        "id": "jLHfnUK_g5vb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab59e47d-296e-4ae4-ff94-b977dd37b383"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'40kLore': [(('whose', 'bolter', 'anyway'), 8),\n",
              "  (('started', 'us', 'examples'), 8),\n",
              "  (('space', 'marine', 'chapter'), 7),\n",
              "  (('kabal', 'black', 'heart'), 7),\n",
              "  (('lo', '’', 'tos'), 7),\n",
              "  (('die', 'paragon', 'knights'), 6),\n",
              "  (('dark', 'age', 'technology'), 4),\n",
              "  (('let', \"'s\", 'say'), 4),\n",
              "  (('``', 'star', 'claimers'), 4),\n",
              "  (('star', 'claimers', \"''\"), 4)],\n",
              " 'AMD_Stock': [(('created', 'subreddit', 'reddit'), 10),\n",
              "  (('subreddit', 'reddit', 'posts'), 10),\n",
              "  (('open', 'redditors', 'posts'), 10),\n",
              "  (('redditors', 'posts', 'well'), 10),\n",
              "  (('posts', 'well', 'please'), 10),\n",
              "  (('well', 'please', 'consider'), 10),\n",
              "  (('please', 'consider', 'subscribing'), 10),\n",
              "  (('consider', 'subscribing', 'find'), 10),\n",
              "  (('subscribing', 'find', 'posts'), 10),\n",
              "  (('find', 'posts', 'helpful'), 10)],\n",
              " 'Anki': [(('``', 'conditional', \"''\"), 7),\n",
              "  (('``', 'field2', \"''\"), 5),\n",
              "  (('\\\\^conditional', 'field1', '/conditional'), 5),\n",
              "  (('conditional', \"''\", 'filled'), 4),\n",
              "  (('font-family', 'simplified', 'arabic'), 3),\n",
              "  (('simplified', 'arabic', 'font-size'), 3),\n",
              "  (('export', 'cards', 'selected'), 3),\n",
              "  (('cards', 'selected', 'browser'), 3),\n",
              "  (('anki', \"'s\", 'code'), 3),\n",
              "  (('conditional', 'field1', '/conditional'), 3)],\n",
              " 'ApexOutlands': [(('arc', 'star', 'arc'), 2),\n",
              "  (('star', 'arc', 'star'), 2),\n",
              "  (('cute', '“', 'mean'), 1),\n",
              "  (('“', 'mean', '..'), 1),\n",
              "  (('mean', '..', 'goo'), 1),\n",
              "  (('..', 'goo', 'goo'), 1),\n",
              "  (('goo', 'goo', 'ga'), 1),\n",
              "  (('goo', 'ga', 'ga'), 1),\n",
              "  (('ga', 'ga', '”'), 1),\n",
              "  (('ga', '”', 'like'), 1)],\n",
              " 'BanGDream': [(('bang', 'dream', 'girls'), 10),\n",
              "  (('boosts', 'score', 'notes'), 9),\n",
              "  (('low', 'effort', 'edit'), 7),\n",
              "  (('effort', 'edit', 'bang'), 7),\n",
              "  (('edit', 'bang', 'dream'), 7),\n",
              "  (('dream', 'girls', 'initial'), 7),\n",
              "  (('girls', 'initial', 'cars'), 7),\n",
              "  (('initial', 'cars', 'either'), 7),\n",
              "  (('cars', 'either', 'make'), 7),\n",
              "  (('either', 'make', 'm.o.v.e'), 7)],\n",
              " 'BrandNewSentence': [(('cool', 'cool', 'cool'), 2),\n",
              "  (('unrealistic', 'de-dun-dun-dun', 'grindr'), 1),\n",
              "  (('de-dun-dun-dun', 'grindr', 'straight'), 1),\n",
              "  (('grindr', 'straight', 'guys'), 1),\n",
              "  (('straight', 'guys', \"n't\"), 1),\n",
              "  (('guys', \"n't\", 'start'), 1),\n",
              "  ((\"n't\", 'start', 'fire'), 1),\n",
              "  (('start', 'fire', 'hate'), 1),\n",
              "  (('fire', 'hate', 'apples'), 1),\n",
              "  (('hate', 'apples', 'avoid'), 1)],\n",
              " 'COVID': [(('covid-19', 'products', 'covid-19'), 6),\n",
              "  (('products', 'covid-19', 'test'), 6),\n",
              "  (('covid-19', 'test', 'kits'), 6),\n",
              "  (('test', 'kits', 'covid-19'), 6),\n",
              "  (('kits', 'covid-19', 'face'), 6),\n",
              "  (('covid-19', 'face', 'masks'), 6),\n",
              "  (('face', 'masks', 'protective'), 6),\n",
              "  (('masks', 'protective', 'isolation'), 6),\n",
              "  (('protective', 'isolation', 'coveralls'), 6),\n",
              "  (('isolation', 'coveralls', 'ppe'), 6)],\n",
              " 'COVID19': [(('coronavirus', 'disease', '2019'), 8),\n",
              "  (('get', 'in-depth', 'ysis'), 6),\n",
              "  (('in-depth', 'ysis', 'covid-19'), 6),\n",
              "  (('ysis', 'covid-19', 'impact'), 6),\n",
              "  (('disease', '2019', 'covid-19'), 4),\n",
              "  (('systematic', 'review', 'meta-ysis'), 4),\n",
              "  (('market', 'get', 'in-depth'), 4),\n",
              "  (('nebraska', 'medical', 'center'), 3),\n",
              "  (('among', 'covid-19', 'patients'), 2),\n",
              "  (('severe', 'covid-19', 'patients'), 2)],\n",
              " 'CanadaCoronavirus': [(('pending|', 'pending', '|self-isolating'), 64),\n",
              "  (('|pending|', 'pending', '|pending'), 57),\n",
              "  (('|pending', '|pending', '|pending'), 53),\n",
              "  (('pending', '|pending', '|self-isolating'), 46),\n",
              "  (('pending', '|pending', '|pending'), 42),\n",
              "  (('``', 'resolved', \"''\"), 32),\n",
              "  (('toronto|', 'pending|', 'pending'), 30),\n",
              "  (('|pending', '|pending', '|self-isolating'), 29),\n",
              "  (('|pending|', 'toronto|', 'pending|'), 28),\n",
              "  (('negative', 'cases', 'daily'), 27)],\n",
              " 'China_Flu': [(('world', 'health', 'organization'), 14),\n",
              "  (('growth', '📈', 'increasing'), 10),\n",
              "  (('total', 'confirmed', 'cases'), 10),\n",
              "  (('new', 'cases', 'coronavirus'), 8),\n",
              "  (('confirmed', 'cases', 'growth'), 8),\n",
              "  (('death', 'rate', 'growth'), 8),\n",
              "  (('available', 'via', 'internet'), 7),\n",
              "  (('last', 'accessed', 'january'), 7),\n",
              "  (('rate', 'growth', 'mixed'), 7),\n",
              "  (('post', 'place', 'general'), 6)],\n",
              " 'Coronavirus': [(('coronavirus', 'death', 'toll'), 29),\n",
              "  (('new', 'coronavirus', 'cases'), 27),\n",
              "  (('new', 'covid-19', 'cases'), 21),\n",
              "  (('new', 'york', 'city'), 20),\n",
              "  (('tests', 'positive', 'coronavirus'), 16),\n",
              "  (('tested', 'positive', 'covid-19'), 15),\n",
              "  (('new', 'cases', 'coronavirus'), 13),\n",
              "  (('test', 'positive', 'covid-19'), 10),\n",
              "  (('tested', 'positive', 'coronavirus'), 10),\n",
              "  (('tests', 'positive', 'covid-19'), 9)],\n",
              " 'CoronavirusCA': [(('daily', 'check-in/personal', 'thread'), 8),\n",
              "  (('2020', 'concerns', 'vents'), 4),\n",
              "  (('concerns', 'vents', 'questions'), 4),\n",
              "  (('vents', 'questions', 'anecdotes'), 4),\n",
              "  (('questions', 'anecdotes', 'personal'), 4),\n",
              "  (('anecdotes', 'personal', 'preparation'), 4),\n",
              "  (('personal', 'preparation', 'understand'), 4),\n",
              "  (('preparation', 'understand', 'stressful'), 4),\n",
              "  (('understand', 'stressful', 'time'), 4),\n",
              "  (('stressful', 'time', 'community'), 4)],\n",
              " 'CoronavirusCirclejerk': [(('average', 'daily', 'number'), 2),\n",
              "  (('daily', 'number', 'deaths'), 2),\n",
              "  (('number', 'deaths', 'per'), 2),\n",
              "  (('deaths', 'per', 'week'), 2),\n",
              "  (('suggestions', 'better', 'unenforceable'), 1),\n",
              "  (('better', 'unenforceable', 'orders'), 1),\n",
              "  (('unenforceable', 'orders', '’'), 1),\n",
              "  (('orders', '’', 'like'), 1),\n",
              "  (('’', 'like', 'casey'), 1),\n",
              "  (('like', 'casey', 'stop'), 1)],\n",
              " 'CoronavirusDownunder': [(('27', '26', '25'), 26),\n",
              "  (('-|', '-|', '-|'), 23),\n",
              "  (('sept', '27', '26'), 23),\n",
              "  (('26', '25', '24'), 23),\n",
              "  (('==', '10', 'days'), 22),\n",
              "  (('10', 'days', 'ago'), 22),\n",
              "  (('24', '23', '22'), 19),\n",
              "  (('25', '24', '23'), 18),\n",
              "  (('aged', 'care', 'facility'), 13),\n",
              "  (('23', '22', '21'), 13)],\n",
              " 'CoronavirusUK': [(('latest', 'r', 'number'), 18),\n",
              "  (('daily', 'deaths', 'total'), 11),\n",
              "  (('nhs', 'test', 'trace'), 9),\n",
              "  (('daily', 'deaths', '0'), 9),\n",
              "  (('deaths', '0', 'total'), 9),\n",
              "  (('test', 'trace', 'ask'), 9),\n",
              "  (('trace', 'ask', 'test'), 9),\n",
              "  (('ask', 'test', 'online'), 9),\n",
              "  (('//www.gov.scot/publications/coronavirus-covid-19-daily-data-for-scotland/',\n",
              "    'daily',\n",
              "    'deaths'),\n",
              "   8),\n",
              "  ((\"'s\", 'latest', 'r'), 8)],\n",
              " 'CoronavirusUS': [(('growth', '📈', 'increasing'), 9),\n",
              "  (('confirmed', 'cases', 'growth'), 8),\n",
              "  (('total', 'confirmed', 'cases'), 8),\n",
              "  (('death', 'rate', 'growth'), 8),\n",
              "  (('cases', 'growth', '📈'), 6),\n",
              "  (('rate', 'growth', 'mixed'), 5),\n",
              "  (('recovery', 'rate', '📉'), 5),\n",
              "  (('update', 'coronavirus', 'growth'), 4),\n",
              "  (('data',\n",
              "    'available',\n",
              "    '//docs.google.com/spreadsheets/d/1w7kqwpsncplcckokvbng8a2nwcn5qo3ydycdcvw38cc/edit'),\n",
              "   4),\n",
              "  (('available',\n",
              "    '//docs.google.com/spreadsheets/d/1w7kqwpsncplcckokvbng8a2nwcn5qo3ydycdcvw38cc/edit',\n",
              "    'gid=503876971range=b4'),\n",
              "   4)],\n",
              " 'CovIdiots': [(('refusing', 'wear', 'mask'), 3),\n",
              "  (('people', \"'s\", 'faces'), 2),\n",
              "  (('tests', 'positive', 'covid-19'), 2),\n",
              "  (('staff', 'nursing', 'home'), 1),\n",
              "  (('nursing', 'home', '19'), 1),\n",
              "  (('home', '19', 'died'), 1),\n",
              "  (('19', 'died', 'told'), 1),\n",
              "  (('died', 'told', 'masks'), 1),\n",
              "  (('told', 'masks', 'would'), 1),\n",
              "  (('masks', 'would', 'scare'), 1)],\n",
              " 'CrackheadCraigslist': [(('deal', 'beauty', 'obsession'), 2),\n",
              "  (('much', 'dude', 'looks'), 1),\n",
              "  (('dude', 'looks', 'like'), 1),\n",
              "  (('looks', 'like', 'jane'), 1),\n",
              "  (('like', 'jane', 'knows'), 1),\n",
              "  (('jane', 'knows', 'wants'), 1),\n",
              "  (('knows', 'wants', '....'), 1),\n",
              "  (('wants', '....', 'looks'), 1),\n",
              "  (('....', 'looks', 'authentic'), 1),\n",
              "  (('looks', 'authentic', 'hmmmm'), 1)],\n",
              " 'EngineeringStudents': [((\"'re\", 'prepare', 'career'), 1),\n",
              "  (('prepare', 'career', 'fairs'), 1),\n",
              "  (('career', 'fairs', 'career'), 1),\n",
              "  (('fairs', 'career', 'fairs'), 1),\n",
              "  (('career', 'fairs', 'job'), 1),\n",
              "  (('fairs', 'job', 'applications'), 1),\n",
              "  (('job', 'applications', 'loom'), 1),\n",
              "  (('applications', 'loom', 'near'), 1),\n",
              "  (('loom', 'near', 'future'), 1),\n",
              "  (('near', 'future', 'resumes'), 1)],\n",
              " 'FigureSkating': [(('russian', 'championships', '2021'), 3),\n",
              "  (('russian', 'figure', 'skating'), 3),\n",
              "  (('rostelecom', 'cup', '2020'), 2),\n",
              "  (('sp', 'russian', 'championships'), 2),\n",
              "  (('sp', 'russian', 'cup'), 2),\n",
              "  (('4th', 'stage', '2020'), 2),\n",
              "  (('best', 'men', \"'s\"), 2),\n",
              "  (('1.', 'yuzuru', 'hanyu'), 2),\n",
              "  (('yuzuru', 'hanyu', '2.'), 2),\n",
              "  (('us', 'nationals', '2021'), 2)],\n",
              " 'Fusion360': [(('fusion', '360', 'beginner'), 2),\n",
              "  (('360', 'beginner', 'tip'), 2),\n",
              "  (('beginner', 'tip', 'clear'), 2),\n",
              "  (('tip', 'clear', 'selections'), 2),\n",
              "  (('clear', 'selections', 'start'), 2),\n",
              "  (('selections', 'start', 'fusion'), 2),\n",
              "  (('start', 'fusion', '360'), 2),\n",
              "  (('fusion', '360', 'increments'), 1),\n",
              "  (('360', 'increments', 'adjust'), 1),\n",
              "  (('increments', 'adjust', 'drag'), 1)],\n",
              " 'Gameboy': [(('despite', 'several', 'modded'), 1),\n",
              "  (('several', 'modded', 'dmgs'), 1),\n",
              "  (('modded', 'dmgs', 'sometimes'), 1),\n",
              "  (('dmgs', 'sometimes', 'like'), 1),\n",
              "  (('sometimes', 'like', 'going'), 1),\n",
              "  (('like', 'going', 'back'), 1),\n",
              "  (('going', 'back', 'humble'), 1),\n",
              "  (('back', 'humble', 'stock'), 1),\n",
              "  (('humble', 'stock', 'game'), 1),\n",
              "  (('stock', 'game', 'boy'), 1)],\n",
              " 'HolUp': [(('p', 'color', 'blind'), 10),\n",
              "  (('color', 'blind', 'test'), 10),\n",
              "  (('blind', 'test', 'p'), 9),\n",
              "  (('test', 'p', 'color'), 9),\n",
              "  (('holup', 'holup', 'holup'), 7),\n",
              "  (('hol', 'hol', 'hol'), 6),\n",
              "  (('wait', 'minute', '....'), 3),\n",
              "  (('minute', '....', 'wait'), 3),\n",
              "  (('wait', 'minute', '...'), 3),\n",
              "  ((\"'s\", 'wrong', \"'s\"), 2)],\n",
              " 'JoeBiden': [(('joe', 'biden', \"'s\"), 7),\n",
              "  (('..', 'joe', 'biden'), 5),\n",
              "  (('joe', 'biden', 'president'), 5),\n",
              "  (('responded', '``', 'true'), 4),\n",
              "  (('``', 'true', \"''\"), 4),\n",
              "  (('covid', 'covid', 'covid'), 4),\n",
              "  (('state', 'covid', 'deaths'), 4),\n",
              "  (('joe', 'biden', 'kamala'), 3),\n",
              "  (('biden', 'kamala', 'harris'), 3),\n",
              "  (('endorse', 'joe', 'biden'), 3)],\n",
              " 'Konosuba': [(('random', 'thought', 'aqua'), 2),\n",
              "  (('megumin', 'drawn', '相川りょう'), 2),\n",
              "  (('took', 'step', 'back'), 2),\n",
              "  (('read', 'comments', 'mom'), 2),\n",
              "  (('megumin', 'vanir', 'wiz'), 1),\n",
              "  (('vanir', 'wiz', 'megumin'), 1),\n",
              "  (('wiz', 'megumin', 'kitsu'), 1),\n",
              "  (('megumin', 'kitsu', 'darkness'), 1),\n",
              "  (('kitsu', 'darkness', 'megumin'), 1),\n",
              "  (('darkness', 'megumin', 'smol'), 1)],\n",
              " 'LivestreamFail': [(('walkthrough', 'gameplay', 'part'), 9),\n",
              "  (('erobb', \"'s\", 'voice'), 3),\n",
              "  ((\"'s\", 'voice', 'cracks'), 3),\n",
              "  (('potty', 'train', 'puppy'), 3),\n",
              "  (('train', 'puppy', 'easily'), 3),\n",
              "  (('puppy', 'easily', 'everything'), 3),\n",
              "  (('easily', 'everything', 'need'), 3),\n",
              "  (('everything', 'need', 'know'), 3),\n",
              "  (('talks', 'first', 'time'), 3),\n",
              "  (('abusing', 'copyright', 'law'), 2)],\n",
              " 'LockdownSkepticism': [(('adjusted', 'odds', 'ratio'), 3),\n",
              "  (('herd', 'immunity', '’'), 2),\n",
              "  (('new', 'york', '’'), 2),\n",
              "  (('vaccine', 'wo', \"n't\"), 2),\n",
              "  (('oxford', 'professor', 'says'), 2),\n",
              "  (('great', 'barrington', 'declaration'), 2),\n",
              "  (('masks', 'social', 'distancing'), 2),\n",
              "  (('people', 'claiming', 'unemployment'), 2),\n",
              "  (('claiming', 'unemployment', 'benefit'), 2),\n",
              "  (('strip', 'club', 'lawsuit'), 2)],\n",
              " 'MensLib': [(('tuesday', 'check', \"'s\"), 1),\n",
              "  (('check', \"'s\", 'everyone'), 1),\n",
              "  ((\"'s\", 'everyone', \"'s\"), 1),\n",
              "  (('everyone', \"'s\", 'mental'), 1),\n",
              "  ((\"'s\", 'mental', 'health'), 1),\n",
              "  (('mental', 'health', 'good'), 1),\n",
              "  (('health', 'good', 'morning'), 1),\n",
              "  (('good', 'morning', 'everyone'), 1),\n",
              "  (('morning', 'everyone', 'welcome'), 1),\n",
              "  (('everyone', 'welcome', 'new'), 1)],\n",
              " 'NintendoSwitch': [(('daily', 'question', 'thread'), 10),\n",
              "  (('animal', 'crossing', 'new'), 9),\n",
              "  (('crossing', 'new', 'horizons'), 8),\n",
              "  (('/r/nintendoswitch', \"'s\", 'daily'), 8),\n",
              "  ((\"'s\", 'daily', 'question'), 8),\n",
              "  ((\"'re\", 'interested', 'becoming'), 8),\n",
              "  (('interested', 'becoming', 'wiki'), 8),\n",
              "  (('becoming', 'wiki', 'contributor'), 8),\n",
              "  (('wiki', 'contributor', '//www.reddit.com/message/compose/'), 8),\n",
              "  (('contributor', '//www.reddit.com/message/compose/', 'to=flapsnapple'), 8)],\n",
              " 'NoLockdownsNoMasks': [(('thailand', 'medical', 'news'), 2),\n",
              "  (('government', 'model', 'suggests'), 1),\n",
              "  (('model', 'suggests', 'u.s.'), 1),\n",
              "  (('suggests', 'u.s.', 'covid-19'), 1),\n",
              "  (('u.s.', 'covid-19', 'cases'), 1),\n",
              "  (('covid-19', 'cases', 'could'), 1),\n",
              "  (('cases', 'could', 'approaching'), 1),\n",
              "  (('could', 'approaching', '100'), 1),\n",
              "  (('approaching', '100', 'million'), 1),\n",
              "  (('100', 'million', 'npr'), 1)],\n",
              " 'NoNewNormal': [(('``', 'covid', \"''\"), 6),\n",
              "  (('``', 'new', 'normal'), 5),\n",
              "  (('new', 'normal', \"''\"), 5),\n",
              "  (('”', '“', '’'), 4),\n",
              "  (('word', '``', 'covid'), 4),\n",
              "  (('people', 'wearing', 'masks'), 3),\n",
              "  (('....', \"''\", \"'m\"), 3),\n",
              "  ((\"''\", \"'m\", 'sorry'), 3),\n",
              "  ((\"'m\", 'sorry', 'forgot'), 3),\n",
              "  (('sorry', 'forgot', 'put'), 3)],\n",
              " 'Norse': [(('viking', 'amulet', 'dragon'), 1),\n",
              "  (('amulet', 'dragon', 'fafnir'), 1),\n",
              "  (('dragon', 'fafnir', 'recreated'), 1),\n",
              "  (('fafnir', 'recreated', 'amulets'), 1),\n",
              "  (('recreated', 'amulets', 'usually'), 1),\n",
              "  (('amulets', 'usually', 'referred'), 1),\n",
              "  (('usually', 'referred', 'viking'), 1),\n",
              "  (('referred', 'viking', 'age'), 1),\n",
              "  (('viking', 'age', 'votive'), 1),\n",
              "  (('age', 'votive', 'thor'), 1)],\n",
              " 'PaymoneyWubby': [(('5', '8', \"''\"), 2),\n",
              "  (('funny', 'clips', 'tonight'), 2),\n",
              "  (('discord', 'trying', 'find'), 1),\n",
              "  (('trying', 'find', 'songs'), 1),\n",
              "  (('find', 'songs', 'played'), 1),\n",
              "  (('songs', 'played', 'stream'), 1),\n",
              "  (('played', 'stream', 'today'), 1),\n",
              "  (('stream', 'today', \"n't\"), 1),\n",
              "  (('today', \"n't\", 'popping'), 1),\n",
              "  ((\"n't\", 'popping', 'media'), 1)],\n",
              " 'Pizza': [(('new', 'york', 'style'), 3),\n",
              "  (('months', 'ago', 'sister'), 1),\n",
              "  (('ago', 'sister', 'tried'), 1),\n",
              "  (('sister', 'tried', 'hand'), 1),\n",
              "  (('tried', 'hand', 'deep-dish'), 1),\n",
              "  (('hand', 'deep-dish', 'birthday'), 1),\n",
              "  (('deep-dish', 'birthday', 'known'), 1),\n",
              "  (('birthday', 'known', 'sub'), 1),\n",
              "  (('known', 'sub', 'would'), 1),\n",
              "  (('sub', 'would', 'taken'), 1)],\n",
              " 'PublicFreakout': [(('black', 'lives', 'matter'), 7),\n",
              "  (('دانلود', 'آهنگ', 'مازندرانی'), 4),\n",
              "  (('new', 'york', 'city'), 3),\n",
              "  (('body', 'cam', 'footage'), 3),\n",
              "  (('p', 'color', 'blind'), 3),\n",
              "  (('color', 'blind', 'test'), 3),\n",
              "  (('دانلود', 'آهنگ', 'کردی'), 3),\n",
              "  (('lives', 'matter', 'protest'), 2),\n",
              "  (('gets', 'racially', 'profiled'), 2),\n",
              "  (('george', 'floyd', 'protests'), 2)],\n",
              " 'SandersForPresident': [(('early', 'voting', 'ends'), 6),\n",
              "  (('bernie', 'sanders', 'president'), 4),\n",
              "  (('bernie', 'sanders', '’'), 4),\n",
              "  (('let', \"'s\", 'get'), 3),\n",
              "  (('bernie', 'sanders', 'nevada'), 3),\n",
              "  (('sen.', 'bernie', 'sanders'), 3),\n",
              "  (('bernie', 'sanders', 'takes'), 3),\n",
              "  (('bernie', 'sanders', 'bernie'), 3),\n",
              "  ((\"''\", 'bernie', 'sanders'), 3),\n",
              "  (('endorses', 'bernie', 'sanders'), 3)],\n",
              " 'TheRealJoke': [(('hitting', 'high', 'note'), 1),\n",
              "  (('high', 'note', 'got'), 1),\n",
              "  (('note', 'got', 'ta'), 1),\n",
              "  (('got', 'ta', 'mother'), 1),\n",
              "  (('ta', 'mother', 'real'), 1),\n",
              "  (('mother', 'real', 'jokes'), 1),\n",
              "  (('real', 'jokes', 'guy'), 1),\n",
              "  (('jokes', 'guy', 'took'), 1),\n",
              "  (('guy', 'took', 'comment'), 1),\n",
              "  (('took', 'comment', 'whole'), 1)],\n",
              " 'TheVampireDiaries': [(('surfer', 'wolf', 'guy'), 4),\n",
              "  (('``', 'agree', \"''\"), 3),\n",
              "  (('’', 'sure', 'one'), 2),\n",
              "  (('’', 'ever', 'seen'), 2),\n",
              "  (('really', 'happened', 'night'), 2),\n",
              "  (('damon', 'killed', 'surfer'), 2),\n",
              "  (('killed', 'surfer', 'wolf'), 2),\n",
              "  (('``', 'accept', 'opinion'), 2),\n",
              "  (('accept', 'opinion', \"''\"), 2),\n",
              "  (('yes', 'klaus', 'caroline'), 1)],\n",
              " 'WTF': [(('sure', 'sneak', 'one'), 1),\n",
              "  (('sneak', 'one', '..'), 1),\n",
              "  (('one', '..', \"'m\"), 1),\n",
              "  (('..', \"'m\", 'march'), 1),\n",
              "  ((\"'m\", 'march', \"'m\"), 1),\n",
              "  (('march', \"'m\", 'going'), 1),\n",
              "  ((\"'m\", 'going', 'need'), 1),\n",
              "  (('going', 'need', 'llama'), 1),\n",
              "  (('need', 'llama', 'firefighters'), 1),\n",
              "  (('llama', 'firefighters', 'surprised'), 1)],\n",
              " 'WindowsMR': [(('hard', 'find', 'replacement'), 1),\n",
              "  (('find', 'replacement', 'controllers'), 1),\n",
              "  (('replacement', 'controllers', 'wmr'), 1),\n",
              "  (('controllers', 'wmr', 'good'), 1),\n",
              "  (('wmr', 'good', 'concept'), 1),\n",
              "  (('good', 'concept', 'replacement'), 1),\n",
              "  (('concept', 'replacement', 'parts'), 1),\n",
              "  (('replacement', 'parts', 'impossible'), 1),\n",
              "  (('parts', 'impossible', 'find'), 1)],\n",
              " 'WitchesVsPatriarchy': [(('``', 'women', 'history'), 3),\n",
              "  (('women', 'history', \"''\"), 3),\n",
              "  (('today', '``', 'women'), 2),\n",
              "  (('given', 'success', '``'), 1),\n",
              "  (('success', '``', 'women'), 1),\n",
              "  (('history', \"''\", 'fictional'), 1),\n",
              "  ((\"''\", 'fictional', 'character'), 1),\n",
              "  (('fictional', 'character', 'entry'), 1),\n",
              "  (('character', 'entry', 'april'), 1),\n",
              "  (('entry', 'april', '1st'), 1)],\n",
              " '[***]og': [(('kodak', 'portra', '400'), 28),\n",
              "  (('kodak', 'gold', '200'), 28),\n",
              "  (('kodak', 'ultramax', '400'), 15),\n",
              "  (('kodak', 'ektar', '100'), 10),\n",
              "  (('portra', '400', 'mamiya'), 10),\n",
              "  (('2.8', 'portra', '400'), 10),\n",
              "  (('canon', 'eos', '3'), 10),\n",
              "  (('olympus', 'xa', 'portra'), 10),\n",
              "  (('pentax', 'p30n', 'vivitar'), 10),\n",
              "  (('28-105mm', 'f/2.8-3.8', 'kodak'), 10)],\n",
              " 'army': [(('fort', 'carson', \"'s\"), 3),\n",
              "  (('wednesday', 'advice', 'thread'), 2),\n",
              "  (('let', 'us', 'know'), 2),\n",
              "  ((\"'best\", \"'worst\", 'mos'), 2),\n",
              "  (('asked', 'question', \"'what\"), 2),\n",
              "  (('question', \"'what\", \"'s\"), 2),\n",
              "  (('mos', 'results', 'follows'), 2),\n",
              "  (('results', 'follows', 'mos|votes'), 2),\n",
              "  (('follows', 'mos|votes', '--'), 2),\n",
              "  (('mos|votes', '--', '--'), 2)],\n",
              " 'bleach': [(('court', 'guard', 'squads'), 12),\n",
              "  (('1st', 'level', 'hell'), 7),\n",
              "  (('13', 'court', 'guard'), 7),\n",
              "  (('court', 'guard', 'squad'), 7),\n",
              "  (('traditional', 'soul', 'reaper'), 7),\n",
              "  (('2nd', 'level', 'hell'), 6),\n",
              "  (('3rd', 'level', 'hell'), 6),\n",
              "  (('guard', 'squads', 'hell'), 6),\n",
              "  (('lowest', 'level', 'hell'), 6),\n",
              "  (('guard', 'squad', 'hell'), 6)],\n",
              " 'brisbane': [(('..', 'cloud', 'report'), 5),\n",
              "  (('prinl', 'place', 'residence'), 4),\n",
              "  (('drink', 'ice', 'cold'), 3),\n",
              "  (('ice', 'cold', 'vb'), 3),\n",
              "  (('cold', 'vb', 'bar'), 3),\n",
              "  (('vb', 'bar', 'fridge'), 3),\n",
              "  (('business', 'activity', 'undertaking'), 3),\n",
              "  (('year', \"'s\", 'ekka'), 2),\n",
              "  ((\"'s\", 'ekka', 'cancelled'), 2),\n",
              "  (('0', 'new', 'covid'), 2)],\n",
              " 'conspiracy': [(('new', 'world', 'order'), 29),\n",
              "  (('black', 'lives', 'matter'), 17),\n",
              "  (('million', 'x200b', 'x200b'), 14),\n",
              "  (('x200b', 'x200b', 'secretary'), 13),\n",
              "  (('military', 'intelligence', 'operation'), 10),\n",
              "  (('royal', 'death', 'racket'), 9),\n",
              "  (('one', 'world', 'government'), 9),\n",
              "  (('thousand', '6', 'months'), 8),\n",
              "  (('10', 'billion', 'trillion'), 8),\n",
              "  (('central', 'intelligence', 'agency'), 8)],\n",
              " 'criminalminds': [(('favourite', 'underrated', 'friendship'), 1),\n",
              "  (('underrated', 'friendship', 'part'), 1),\n",
              "  (('friendship', 'part', '2'), 1),\n",
              "  (('part', '2', '//www.reddit.com/poll/k2rntc'), 1),\n",
              "  (('2', '//www.reddit.com/poll/k2rntc', 'elle'), 1),\n",
              "  (('//www.reddit.com/poll/k2rntc', 'elle', 'emily'), 1),\n",
              "  (('elle', 'emily', 'felt'), 1),\n",
              "  (('emily', 'felt', 'like'), 1),\n",
              "  (('felt', 'like', 'screen'), 1),\n",
              "  (('like', 'screen', 'time'), 1)],\n",
              " 'donaldtrump': [(('donald', 'j.', 'trump'), 27),\n",
              "  (('j.', 'trump', '``'), 22),\n",
              "  (('joe', 'biden', \"'s\"), 8),\n",
              "  (('2020', 'potus', 'schedule'), 7),\n",
              "  (('team', 'trump', '``'), 6),\n",
              "  (('president', 'trump', \"'s\"), 6),\n",
              "  (('president', 'donald', 'trump'), 6),\n",
              "  (('black', 'lives', 'matter'), 6),\n",
              "  (('dan', 'bongino', 'show®'), 5),\n",
              "  (('president', 'donald', 'j.'), 5)],\n",
              " 'gundeals': [(('ship', 'tax', 'az'), 3),\n",
              "  (('coupon', 'code', '``'), 3),\n",
              "  (('rock', 'island', 'armory'), 2),\n",
              "  (('sds', 'imports', '1911'), 2),\n",
              "  (('imports', '1911', 'duty'), 2),\n",
              "  (('grand', 'power', 'stribog'), 2),\n",
              "  (('power', 'stribog', '9mm'), 2),\n",
              "  (('stribog', '9mm', 'gpsp9a1'), 2),\n",
              "  (('free', 'ship', 'tax'), 2),\n",
              "  (('chinese', 'type', '56'), 2)],\n",
              " 'intermittentfasting': [(('dr.', 'jason', 'fung'), 3),\n",
              "  (('intermittent', 'fasting', 'longevity'), 2),\n",
              "  (('trouble', 'sleeping', '’'), 2),\n",
              "  (('seeing', 'physical', 'changes'), 2),\n",
              "  (('physical', 'changes', '’'), 2),\n",
              "  (('daily', 'intermittent', 'fasting'), 1),\n",
              "  (('intermittent', 'fasting', 'thread'), 1),\n",
              "  (('fasting', 'thread', 'share'), 1),\n",
              "  (('thread', 'share', 'daily'), 1),\n",
              "  (('share', 'daily', 'intermittent'), 1)],\n",
              " 'l4d2': [(('new', 'players', 'like'), 1),\n",
              "  (('players', 'like', '’'), 1),\n",
              "  (('like', '’', 'learnding'), 1),\n",
              "  (('’', 'learnding', 'congratulations'), 1),\n",
              "  (('learnding', 'congratulations', 'u/-leblanc-'), 1),\n",
              "  (('congratulations', 'u/-leblanc-', '...'), 1),\n",
              "  (('u/-leblanc-', '...', 'every'), 1),\n",
              "  (('...', 'every', 'match'), 1),\n",
              "  (('every', 'match', 'bad'), 1),\n",
              "  (('match', 'bad', 'guy'), 1)],\n",
              " 'opensource': [(('lemmy', 'open', 'source'), 1),\n",
              "  (('open', 'source', 'decentralized'), 1),\n",
              "  (('source', 'decentralized', 'reddit'), 1),\n",
              "  (('decentralized', 'reddit', 'alternative'), 1),\n",
              "  (('reddit', 'alternative', 'release'), 1),\n",
              "  (('alternative', 'release', 'v0.6.0'), 1),\n",
              "  (('release', 'v0.6.0', 'avatars'), 1),\n",
              "  (('v0.6.0', 'avatars', 'email'), 1),\n",
              "  (('avatars', 'email', 'notifications'), 1),\n",
              "  (('email', 'notifications', 'whole'), 1)],\n",
              " 'playboicarti': [(('\\u200e', '\\u200e', '\\u200e'), 88),\n",
              "  (('whole', 'lotta', 'red'), 34),\n",
              "  (('’', 'gon', 'na'), 19),\n",
              "  (('bruh', 'really', 'happening'), 16),\n",
              "  (('gon', 'na', 'get'), 13),\n",
              "  (('really', 'happening', 'bruh'), 12),\n",
              "  (('happening', 'bruh', 'really'), 12),\n",
              "  (('’', 'think', '’'), 10),\n",
              "  (('pi', '’', 'erre'), 9),\n",
              "  (('range', 'rover', 'sport'), 8)],\n",
              " 'politics': [((\"'ll\", 'right', 'eventually'), 1),\n",
              "  (('right', 'eventually', 'trump'), 1),\n",
              "  (('eventually', 'trump', 'insists'), 1),\n",
              "  (('trump', 'insists', 'coronavirus'), 1),\n",
              "  (('insists', 'coronavirus', 'disappear'), 1),\n",
              "  (('coronavirus', 'disappear', 'citing'), 1),\n",
              "  (('disappear', 'citing', 'evidence'), 1),\n",
              "  (('citing', 'evidence', 'obama'), 1),\n",
              "  (('evidence', 'obama', 'says'), 1),\n",
              "  (('obama', 'says', 'democrat'), 1)],\n",
              " 'razer': [(('razer', 'sea-invitational', '2020'), 7),\n",
              "  (('sea-invitational', '2020', 'dota2'), 5),\n",
              "  (('hdmi', '1', 'dp'), 3),\n",
              "  (('lower', 'bracket', 'finals'), 3),\n",
              "  (('899', 'razer', 'raptor'), 2),\n",
              "  (('1', 'hdmi', '1'), 2),\n",
              "  (('1', 'dp', '3.5mm'), 2),\n",
              "  (('dp', '3.5mm', 'audio'), 2),\n",
              "  (('3.5mm', 'audio', 'jack'), 2),\n",
              "  (('could', 'ever', 'see'), 2)],\n",
              " 'rutgers': [(('intro', 'financial', 'accounting'), 3),\n",
              "  (('writing', 'labor', 'studies'), 3),\n",
              "  (('labor', 'studies', 'employment'), 3),\n",
              "  (('studies', 'employment', 'relations'), 3),\n",
              "  (('minor', 'developmental', 'psychology'), 2),\n",
              "  (('intro', 'stats', 'business'), 2),\n",
              "  (('stats', 'business', 'intro'), 2),\n",
              "  (('business', 'intro', 'financial'), 2),\n",
              "  (('refund', 'term', 'bill'), 2),\n",
              "  (('know', 'easy', 'upper'), 2)],\n",
              " 'sony': [(('sony', 'ifa', '2020'), 3),\n",
              "  (('looking', 'forward', 'sony'), 2),\n",
              "  (('forward', 'sony', 'ifa'), 2),\n",
              "  (('ifa', '2020', 'press'), 1),\n",
              "  (('2020', 'press', 'conference'), 1),\n",
              "  (('press', 'conference', 'going'), 1),\n",
              "  (('conference', 'going', 'ahead'), 1),\n",
              "  (('going', 'ahead', '’'), 1),\n",
              "  (('ahead', '’', 'sony'), 1),\n",
              "  (('’', 'sony', 'modify'), 1)],\n",
              " 'sportsbook': [(('//www.reddit.com/r/sportsbook/search', 'q=title', '3a'),\n",
              "   28),\n",
              "  (('22restrict_sr=onsort=newt=all',\n",
              "    '//www.reddit.com/r/sportsbook/search',\n",
              "    'q=title'),\n",
              "   21),\n",
              "  (('//www.reddit.com/r/sportsbook/wiki/sportsbooks',\n",
              "    '//discordapp.com/invite/0z5fkengsblokq4s',\n",
              "    '//www.reddit.com/r/sportsbook/search'),\n",
              "   7),\n",
              "  (('//discordapp.com/invite/0z5fkengsblokq4s',\n",
              "    '//www.reddit.com/r/sportsbook/search',\n",
              "    'q=title'),\n",
              "   7),\n",
              "  (('q=title', '3a', '22general+discussion'), 7),\n",
              "  (('3a', '22general+discussion', '2fquestions+biweekly'), 7),\n",
              "  (('22general+discussion',\n",
              "    '2fquestions+biweekly',\n",
              "    '22restrict_sr=onsort=newt=all'),\n",
              "   7),\n",
              "  (('2fquestions+biweekly',\n",
              "    '22restrict_sr=onsort=newt=all',\n",
              "    '//www.reddit.com/r/sportsbook/search'),\n",
              "   7),\n",
              "  (('q=title', '3a', '22futures+monthly'), 7),\n",
              "  (('3a', '22futures+monthly', '22restrict_sr=onsort=newt=all'), 7)],\n",
              " 'touhou': [(('touhou', 'music', 'thread'), 18),\n",
              "  (('doujin', 'mix', 'touhou'), 9),\n",
              "  (('gensokyo', 'would', 'rather'), 6),\n",
              "  (('daily', 'gensokyo', 'would'), 5),\n",
              "  (('ahead', 'thought', 'play'), 5),\n",
              "  (('//old.reddit.com/r/touhou/comments/einuiz/touhou_music_thread_index/',\n",
              "    'touhou',\n",
              "    'music'),\n",
              "   5),\n",
              "  ((\"''\", 'pokemon', 'bw/touhou'), 5),\n",
              "  (('pokemon', 'bw/touhou', 'style'), 5),\n",
              "  (('bw/touhou', 'style', 'touhou'), 5),\n",
              "  (('..', 'daily', 'taishi-sama'), 4)],\n",
              " 'virginvschad': [(('chadcano', 'season', '4'), 3),\n",
              "  (('season', '4', 'episode'), 2),\n",
              "  (('names', 'thad', 'tracy'), 2),\n",
              "  (('would', 'like', 'announce'), 2),\n",
              "  (('official', 'vvc', 'height'), 1),\n",
              "  (('vvc', 'height', 'incel'), 1),\n",
              "  (('height', 'incel', 'bruce'), 1),\n",
              "  (('incel', 'bruce', '..'), 1),\n",
              "  (('bruce', '..', 'nonexistentad'), 1),\n",
              "  (('..', 'nonexistentad', 'behold'), 1)],\n",
              " 'wicked_edge': [(('full', 'show', 'online'), 6),\n",
              "  (('show', 'online', 'free'), 6),\n",
              "  (('cyril', 'r.', 'salter'), 5),\n",
              "  (('bulgari', 'man', 'black'), 3),\n",
              "  (('r.', 'salter', 'french'), 3),\n",
              "  (('salter', 'french', 'vetiver'), 3),\n",
              "  (('crs', 'french', 'vetiver'), 3),\n",
              "  (('english', 'subbed', 'watch'), 3),\n",
              "  (('zingari', 'man', 'blacksmith'), 2),\n",
              "  (('obligatory', 'sotd', 'pic'), 2)],\n",
              " 'worldbuilding': [(('little', 'lore', 'game'), 10),\n",
              "  (('lore', 'game', 'coming'), 10),\n",
              "  (('game', 'coming', 'small'), 10),\n",
              "  (('coming', 'small', 'things'), 10),\n",
              "  (('small', 'things', 'worlds'), 10),\n",
              "  (('things', 'worlds', 'challenges'), 10),\n",
              "  (('worlds', 'challenges', 'posted'), 10),\n",
              "  (('trying', 'get', 'something'), 10),\n",
              "  (('get', 'something', 'world'), 10),\n",
              "  (('something', 'world', 'nothing'), 10)],\n",
              " 'xqcow': [(('lego', 'harry', 'potter'), 10),\n",
              "  (('harry', 'potter', 'babyrage'), 10),\n",
              "  (('potter', 'babyrage', 'lego'), 9),\n",
              "  (('babyrage', 'lego', 'harry'), 9),\n",
              "  (('posting', 'widepeepohappy', 'xqc'), 8),\n",
              "  (('widepeepohappy', 'xqc', 'stares'), 6),\n",
              "  (('xqc', 'stares', 'camera'), 6),\n",
              "  (('stares', 'camera', 'says'), 6),\n",
              "  (('camera', 'says', 'widepeepohappy'), 6),\n",
              "  (('says', 'widepeepohappy', 'day'), 3)]}"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4yJhYmqJEmso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8E010UbQyML"
      },
      "source": [
        "## P1.2 - Answering questions with pandas (15 marks)\n",
        "\n",
        "In this question, your task is to use pandas to answer questions about the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZmG2VIYQ93I"
      },
      "source": [
        "### P1.2.1 - Authors that post highly commented posts (3 marks)\n",
        "\n",
        "Find the top 1000 most commented posts. Then, obtain the names of the authors that have at least 3 posts among these posts.\n",
        "\n",
        "**What to implement:** Implement a function `find_popular_authors(df)` that takes as input the original dataframe and returns a list strings, where each string is the name of authors that satisfy the above criteria."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def find_popular_authors(df):\n",
        "    # Sort the posts by number of comments in descending order\n",
        "    sorted_df = df.sort_values('num_comments', ascending=False)\n",
        "\n",
        "    # Take the top 1000 most commented posts\n",
        "    top_1000_df = sorted_df.head(1000)\n",
        "\n",
        "    # Count the number of posts per author\n",
        "    author_counts = top_1000_df['author'].value_counts()\n",
        "\n",
        "    # Filter the authors that have at least 3 posts among the top 1000\n",
        "    popular_authors = author_counts[author_counts >= 3].index.tolist()\n",
        "\n",
        "    return popular_authors"
      ],
      "metadata": {
        "id": "URKIW6oMvrYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "find_popular_authors(df)"
      ],
      "metadata": {
        "id": "tmsJyZ1_xpGG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64979fd4-c36c-40a5-e8b6-e55fd447de84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['AutoModerator',\n",
              " 'r[***]og',\n",
              " 'jigsawmap',\n",
              " 'Salramm01',\n",
              " 'HippolasCage',\n",
              " 'FunPeach0',\n",
              " 'iSlingShlong',\n",
              " 'Stoaticor',\n",
              " 'kevinmrr',\n",
              " 'ratioetlogicae',\n",
              " 'None',\n",
              " 'harushiga',\n",
              " 'tefunka',\n",
              " 'SlobBarker',\n",
              " 'stargem5',\n",
              " 'AristonD',\n",
              " 'werdmouf',\n",
              " 'Cross_Ange',\n",
              " 'samzz41',\n",
              " 'itsreallyreallytrue',\n",
              " 'SUPERGUESSOUS',\n",
              " 'Frocharocha',\n",
              " 'habichuelacondulce',\n",
              " 'CantStopPoppin',\n",
              " 'Allstarhit',\n",
              " 'theitguyforever',\n",
              " 'rebooted_life_42',\n",
              " 'Zhana-Aul',\n",
              " 'Not4Reel',\n",
              " 'Jellyrollrider',\n",
              " 'NYLaw',\n",
              " 'MakeItRainSheckels',\n",
              " 'TurtleFacts72',\n",
              " 'Defie-LOH-Gic',\n",
              " 'Typoqueen00',\n",
              " 'imagepoem',\n",
              " 'nycsellit4me',\n",
              " 'madman320',\n",
              " 'mythrowawaybabies',\n",
              " 'kogeliz',\n",
              " 'strngerdngermaus',\n",
              " 'Kinmuan',\n",
              " 'AllisonGator',\n",
              " 'Antiliani',\n",
              " 'vizard673',\n",
              " 'notpreposterous',\n",
              " 'BanDerUh',\n",
              " 'dukey',\n",
              " 'BebeFanMasterJ',\n",
              " 'Fr1sk3r',\n",
              " 'Gambit08',\n",
              " 'XDitto',\n",
              " 'elt0p0',\n",
              " 'twistedlogicx',\n",
              " 'TAKEitTOrCIRCLEJERK',\n",
              " 'Ramy_',\n",
              " 'tacolben',\n",
              " 'Morihando',\n",
              " '2020c[***]er[***]',\n",
              " 'dunphish64',\n",
              " 'apocalypticalley',\n",
              " 'dsbwayne',\n",
              " 'schuey_08',\n",
              " 'blacked_lover',\n",
              " 'stealthyfrog',\n",
              " 'TheFearlessWarrior',\n",
              " 'akarim5847',\n",
              " 'invertedparado[***]',\n",
              " 'faab64',\n",
              " 'bgny',\n",
              " 'ufgman',\n",
              " 'ReginaldJohnston',\n",
              " 'Singularitytracker',\n",
              " 'Lshim',\n",
              " 'chakalakasp',\n",
              " 'Romano16',\n",
              " 'foodforthinks',\n",
              " 'Mahomeboy_',\n",
              " 'lilmcfuggin',\n",
              " 'MisterT12',\n",
              " 'Majnum',\n",
              " 'CLO_Junkie',\n",
              " 'epiphanyx99',\n",
              " 'MrRoxx',\n",
              " 'Saibasaurus',\n",
              " 'KatieAllTheTime',\n",
              " 'boomerpro',\n",
              " 'le_br1t',\n",
              " 'hilltopye',\n",
              " 'Lost_Distribution546',\n",
              " 'PlenitudeOpulence',\n",
              " 'Wagamaga',\n",
              " 'OldFashionedJizz',\n",
              " 'WorkTomorrow',\n",
              " 'mouthofreason',\n",
              " 'hildebrand_rarity',\n",
              " 'DaFunkJunkie',\n",
              " 'SemperPereunt',\n",
              " 'Leg_holes',\n",
              " 'bemani4u',\n",
              " 'Playaguy',\n",
              " 'jollygreenscott91',\n",
              " 'allicat83',\n",
              " 'lanqian',\n",
              " 'into_the_[***]e',\n",
              " 'UpNDownCan',\n",
              " 'puppuli',\n",
              " 'johnruby',\n",
              " 'Madd-Nigrulo',\n",
              " 'OgranismAtWork',\n",
              " 'cdillon42',\n",
              " 'oliver_21',\n",
              " 'somnifacientsawyer',\n",
              " '[***]reader']"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jrl0kq09dxrp"
      },
      "source": [
        "### P1.2.2 - Distribution of posts per weekday (5 marks)\n",
        "\n",
        "Find the percentage of posts that were posted in each weekday (Monday, Tuesday, etc.). You can use an external calendar or you can use any functionality for dealing with dates available in pandas. \n",
        "\n",
        "**What to implement:** A function `get_weekday_post_distribution(df)` that takes as input the original dataframe and returns a dictionary of the form (the values are made up):\n",
        "\n",
        "```\n",
        "{'Monday': '14%',\n",
        "'Tuesday': '23%', \n",
        "...\n",
        "}\n",
        "```\n",
        "\n",
        "Note that you must only return two decimals, and you must include the percentage sign in the output dictionary. \n",
        "\n",
        "Note that in dictionaries order is not preserved, so the order in which it gets printed will not matter. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_weekday_post_distribution(df):\n",
        "  # your answer here\n",
        "  # Convert 'created_utc' column to datetime format\n",
        "  df['posted_at'] = pd.to_datetime(df['posted_at'], infer_datetime_format=True)\n",
        "\n",
        "  # Extract weekday information\n",
        "  df['weekday'] = df['posted_at'].dt.day_name()\n",
        "\n",
        "  # Count the number of posts for each weekday\n",
        "  weekday_counts = df['weekday'].value_counts()\n",
        "\n",
        "  # Calculate the percentage of posts for each weekday\n",
        "  total_posts = weekday_counts.sum()\n",
        "  weekday_percentages = (weekday_counts / total_posts * 100).round(2)\n",
        "\n",
        "  # Create the output dictionary\n",
        "  output_dict = {}\n",
        "  for weekday in weekday_percentages.index:\n",
        "      percentage_str = '{:.2f}%'.format(weekday_percentages[weekday])\n",
        "      output_dict[weekday] = percentage_str\n",
        "\n",
        "    \n",
        "  return output_dict\n"
      ],
      "metadata": {
        "id": "Aj_2ss9jy9WC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_weekday_post_distribution(df)"
      ],
      "metadata": {
        "id": "5FUpOzgN1t3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91d1cc34-1ed5-4337-f715-4afd7bb0a96d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Wednesday': '14.89%',\n",
              " 'Friday': '14.79%',\n",
              " 'Thursday': '14.75%',\n",
              " 'Tuesday': '14.54%',\n",
              " 'Monday': '14.31%',\n",
              " 'Saturday': '13.76%',\n",
              " 'Sunday': '12.96%'}"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVj1WikSUPjO"
      },
      "source": [
        "### P1.2.3 - The 100 most passionate redditors (7 marks)\n",
        "\n",
        "We would like to know which are the 100 redditors (`author` column) that are most passionate. We will measure this by checking, for each redditor, the ratio at which they use adjectives. This ratio will be computed by dividing number of adjectives by the total number of words each redditor used. The analysis will only consider redditors that have written at least 1000 words.\n",
        "\n",
        "**What to implement:** A function called `get_passionate_redditors(df)` that takes as input the original dataframe and returns a list of the top 100 redditors (authors) by the ratio at which they use adjectives considering both the `title` and `selftext` columns. The returned list should be a list of tuples, where each inner tuple has two elements: the redditor (author) name, and the ratio of adjectives they used. The returned list should be sorted by adjective ratio in descending order (highest first). Only redditors that wrote more than 1000 words should be considered. You should use `nltk`'s `word_tokenize` and `pos_tag` functions to tokenize and find adjectives. You do not need to do any preprocessing like stopword removal, lemmatization or stemming."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#def get_passionate_redditors(df):\n",
        "  # your answer here\n",
        "import nltk\n",
        "from nltk import word_tokenize, pos_tag\n",
        "from collections import defaultdict\n",
        "\n",
        "def get_passionate_redditors(df):\n",
        "    redditor_dict = defaultdict(lambda: [0, 0]) # dictionary to store count of adjectives and total words\n",
        "    for index, row in df.iterrows():\n",
        "        # combine title and selftext columns into one string\n",
        "        text = row['title'] + ' ' + row['selftext'] \n",
        "        tokens = word_tokenize(text) # tokenize the text\n",
        "        tags = pos_tag(tokens) # part of speech tagging\n",
        "        adjectives = [tag[0] for tag in tags if tag[1] in ['JJ', 'JJR', 'JJS']] # extract adjectives\n",
        "        num_adjectives = len(adjectives)\n",
        "        num_words = len(tokens)\n",
        "        author = row['author']\n",
        "        redditor_dict[author][0] += num_adjectives\n",
        "        redditor_dict[author][1] += num_words\n",
        "        passionate_redditors = []\n",
        "    for author, counts in redditor_dict.items():\n",
        "        if counts[1] >= 1000: # only consider redditors with more than 1000 words\n",
        "            adj_ratio = counts[0] / counts[1] # calculate adjective ratio\n",
        "            passionate_redditors.append((author, adj_ratio))\n",
        "            passionate_redditors.sort(key=lambda x: x[1], reverse=True) # sort by adjective ratio\n",
        "    return passionate_redditors[:100] # return top 100 redditors\n"
      ],
      "metadata": {
        "id": "yjfpDjS2oPzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "def get_passionate_redditors(df):\n",
        "    # Tokenize the full_text column and tag the parts of speech\n",
        "    df['full_text_tokenized'] = df.apply(lambda row: nltk.word_tokenize(row['title'] + '. ' + row['selftext']), axis=1)\n",
        "    df['pos_tags'] = df.apply(lambda row: nltk.pos_tag(row['full_text_tokenized']), axis=1)\n",
        "\n",
        "    # Calculate the number of adjectives and total words for each author\n",
        "    author_counts = {}\n",
        "    for _, row in df.iterrows():\n",
        "        author = row['author']\n",
        "        if author not in author_counts:\n",
        "            author_counts[author] = {'adjectives': 0, 'total_words': 0}\n",
        "        for word, pos in row['pos_tags']:\n",
        "            if pos.startswith('JJ'):\n",
        "                author_counts[author]['adjectives'] += 1\n",
        "            author_counts[author]['total_words'] += 1\n",
        "\n",
        "    # Calculate adjective ratio for each author and filter by word count\n",
        "    passionate_redditors = []\n",
        "    for author, counts in author_counts.items():\n",
        "        if counts['total_words'] >= 1000:\n",
        "            adj_ratio = counts['adjectives'] / counts['total_words']\n",
        "            passionate_redditors.append((author, adj_ratio))\n",
        "\n",
        "    # Sort by adjective ratio in descending order and return the top 100\n",
        "    passionate_redditors.sort(key=lambda x: x[1], reverse=True)\n",
        "    return passionate_redditors[:100]\n"
      ],
      "metadata": {
        "id": "5nTdEt52rXxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_passionate_redditors(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5z99U19k6sI",
        "outputId": "b85b3812-d2ce-4046-e2cf-c4b040b5af42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('OhanianIsTheBest', 0.14718787395293179),\n",
              " ('healrstreettalk', 0.13043478260869565),\n",
              " ('FreedomBoners', 0.12429565793834936),\n",
              " ('factfind', 0.11272504091653028),\n",
              " ('Travis-Cole', 0.10551181102362205),\n",
              " ('SecretAgentIceBat', 0.10299145299145299),\n",
              " ('fullbloodedwhitemale', 0.10263929618768329),\n",
              " ('GeAlltidUpp', 0.09623624782860452),\n",
              " ('backpackwayne', 0.09612277867528271),\n",
              " ('Tripmooney', 0.09552495697074011),\n",
              " ('mission_improbables', 0.09512719455392332),\n",
              " ('nyello-2000', 0.09355067328136074),\n",
              " ('EMB1981', 0.09316101238556812),\n",
              " ('greyuniwave', 0.0912906610703043),\n",
              " ('th3allyK4t', 0.09067357512953368),\n",
              " ('Venus230', 0.08927108927108927),\n",
              " ('35quai', 0.08869565217391304),\n",
              " ('kent_k', 0.08856345885634588),\n",
              " ('120inn[***]', 0.08775137111517367),\n",
              " ('theinfinitelight', 0.08732534930139721),\n",
              " ('notinferno', 0.08640776699029126),\n",
              " ('rrixham', 0.08589458922519574),\n",
              " ('Ninten-Doh', 0.08562197092084006),\n",
              " ('kay278', 0.08499005964214712),\n",
              " ('secretymology', 0.08494031221303948),\n",
              " ('society0', 0.08418891170431211),\n",
              " ('Zendexor', 0.08397841895378841),\n",
              " ('spirtomb1831', 0.0831889081455806),\n",
              " ('III_lll', 0.08286176232821342),\n",
              " ('anon7935678', 0.08282379099499722),\n",
              " ('allofusahab', 0.08280254777070063),\n",
              " ('___TheKid___', 0.08263695450324977),\n",
              " ('BornOnADifCloud', 0.08248232521602514),\n",
              " ('sbpotdbot', 0.08235294117647059),\n",
              " ('AnakinWayneII', 0.08177044261065267),\n",
              " ('pooheygirl', 0.08177044261065267),\n",
              " ('The_In-Betweener', 0.08165272995573045),\n",
              " ('snorken123', 0.08112582781456953),\n",
              " ('CommonEmployment2', 0.08085501858736059),\n",
              " ('XDitto', 0.08006814310051108),\n",
              " ('clemaneuverers', 0.07998242144583607),\n",
              " ('NewTsahi', 0.07980900409276943),\n",
              " ('jmou3dxf', 0.07977207977207977),\n",
              " ('cialu', 0.0796221322537112),\n",
              " ('Asshole411', 0.07875457875457875),\n",
              " ('yellowsnow2', 0.07842227378190256),\n",
              " ('FringeCenterPodcast', 0.07826747720364742),\n",
              " ('kit8642', 0.07796340493237867),\n",
              " ('venCiere', 0.07719799857040743),\n",
              " ('Stoaticor', 0.07718696397941681),\n",
              " ('bionista', 0.07709604882749758),\n",
              " ('arthurmilchior', 0.07685916078105526),\n",
              " ('dontbuyanylogos', 0.0763019781994348),\n",
              " ('reddit_loves_pedos', 0.07571857571857572),\n",
              " ('Long_on_AMD', 0.07569296375266525),\n",
              " ('BlindingTwilight', 0.07557397959183673),\n",
              " ('samueldon2020', 0.07521739130434782),\n",
              " ('AutoModerator', 0.07461240310077519),\n",
              " ('Kinmuan', 0.07452135493372607),\n",
              " ('Pretty_iin_Pink', 0.07451855986603405),\n",
              " ('xrangegod1', 0.0738488271068636),\n",
              " ('Cross_Ange', 0.07380298871607198),\n",
              " ('CuteBananaMuffin', 0.07348799785436502),\n",
              " ('JeopardyGreen', 0.07323452484742807),\n",
              " ('SlobBarker', 0.07214137214137215),\n",
              " ('DevilTrigger789', 0.0718562874251497),\n",
              " ('Benster_ninja', 0.07169092640457081),\n",
              " ('north0east', 0.07164179104477612),\n",
              " ('scamaltert', 0.07147673634524612),\n",
              " ('truthesda', 0.07085168869309838),\n",
              " ('Johnny21X', 0.0703971119133574),\n",
              " ('blink3892938', 0.07033639143730887),\n",
              " ('Rairaijin', 0.06923269182704324),\n",
              " ('0naptoon', 0.06881313131313131),\n",
              " ('Grtrshop', 0.06869479882237488),\n",
              " ('BK-Vatras', 0.06867998051631759),\n",
              " ('Facts-Over-Opinion', 0.06832871652816251),\n",
              " ('bgny', 0.06820066334991708),\n",
              " ('austria9000', 0.06723484848484848),\n",
              " ('Collective1985', 0.06719367588932806),\n",
              " ('bluesun100', 0.06705118961788031),\n",
              " ('10100011a10100011a', 0.06664170722575814),\n",
              " ('OmniusQubus', 0.06629032258064516),\n",
              " ('azagitova', 0.0657606313020605),\n",
              " ('HorsesPlease', 0.0651942522618414),\n",
              " ('coronaobserver', 0.06508047585724283),\n",
              " ('DeadEndFred', 0.06431852986217458),\n",
              " ('BeardedJho', 0.06396761133603239),\n",
              " ('tari101190', 0.06392737697085524),\n",
              " ('Mrexreturns', 0.063915857605178),\n",
              " ('jdd7690', 0.06385448916408669),\n",
              " ('Cloud9Shopper', 0.06382978723404255),\n",
              " ('FloundersEdition', 0.06374133949191686),\n",
              " ('balbs10', 0.06332179930795848),\n",
              " ('ijoinedfor[***]Cx', 0.06314063140631407),\n",
              " ('superiorpanda', 0.06307583274273565),\n",
              " ('koolman631', 0.06229508196721312),\n",
              " ('LBC_Black_Cross', 0.061738002594033725),\n",
              " ('Daeani', 0.061677631578947366),\n",
              " ('DanishViking81', 0.06140788816774838)]"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_passionate_redditors(df)"
      ],
      "metadata": {
        "id": "7ddl-35Trg2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ac1fbef-adec-4b56-98ad-2d5c613efe25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('FreedomBoners', 0.12681912681912683),\n",
              " ('EMB1981', 0.09973753280839895),\n",
              " ('backpackwayne', 0.09927710843373494),\n",
              " ('Travis-Cole', 0.09859154929577464),\n",
              " ('SecretAgentIceBat', 0.09746954076850985),\n",
              " ('mission_improbables', 0.08791448516579406),\n",
              " ('GeAlltidUpp', 0.08745369624738283),\n",
              " ('factfind', 0.08734804142278253),\n",
              " ('rrixham', 0.08634933123524784),\n",
              " ('yellowsnow2', 0.0854119425547997),\n",
              " ('greyuniwave', 0.0852627710400588),\n",
              " ('The_In-Betweener', 0.07946278679350867),\n",
              " ('Stoaticor', 0.07934238741958542),\n",
              " ('120inn[***]', 0.07908163265306123),\n",
              " ('clemaneuverers', 0.07362377575143532),\n",
              " ('dontbuyanylogos', 0.07287093942054433),\n",
              " ('SlobBarker', 0.07096774193548387),\n",
              " ('reddit_loves_pedos', 0.07011915673693858),\n",
              " ('CuteBananaMuffin', 0.0700209927541139),\n",
              " ('CommonEmployment2', 0.06983511154219205),\n",
              " ('Benster_ninja', 0.06855357471053115),\n",
              " ('BlindingTwilight', 0.0669710806697108),\n",
              " ('AutoModerator', 0.066797161732322),\n",
              " ('Kinmuan', 0.06666666666666667),\n",
              " ('Pretty_iin_Pink', 0.06634544106745738),\n",
              " ('blink3892938', 0.06599713055954089),\n",
              " ('10100011a10100011a', 0.06521739130434782),\n",
              " ('BK-Vatras', 0.06456456456456457),\n",
              " ('venCiere', 0.06412478336221837),\n",
              " ('Rairaijin', 0.06375442739079103),\n",
              " ('coronaobserver', 0.062448644207066556),\n",
              " ('tari101190', 0.0618185455636691),\n",
              " ('balbs10', 0.06149050722584302),\n",
              " ('truthesda', 0.06073619631901841),\n",
              " ('azagitova', 0.060626702997275204),\n",
              " ('HorsesPlease', 0.05948242564696794),\n",
              " ('Cross_Ange', 0.05914221218961625),\n",
              " ('bgny', 0.05903814262023217),\n",
              " ('ijoinedfor[***]Cx', 0.058909444985394355),\n",
              " ('patiencetruth', 0.05808903365906623),\n",
              " ('Przemek0980', 0.057988523104802174),\n",
              " ('WearyThanks', 0.0570430733410943),\n",
              " ('topcreamboi', 0.05696636925188744),\n",
              " ('OmniusQubus', 0.05665930831493746),\n",
              " ('shylock92008', 0.055900621118012424),\n",
              " ('MaxDemonNoir', 0.054558011049723756),\n",
              " ('seanspeaks77', 0.05388331475287997),\n",
              " ('FloundersEdition', 0.049955396966993755),\n",
              " ('HoxtonNoir', 0.04941176470588235),\n",
              " ('Accelerator231', 0.04154929577464789),\n",
              " ('ArrancarIsaoMizota', 0.040671400903808906),\n",
              " ('Scyllarious', 0.04028487947406866),\n",
              " ('2012ronpaul2012', 0.034411562284927734),\n",
              " ('x23b1', 0.023484201537147736)]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## P1.3 Ethics (10 marks)"
      ],
      "metadata": {
        "id": "jQKJ4zc9UyOc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsAF9jpblJLp"
      },
      "source": [
        "Imagine you are **the head of a data mining company** that needs to use the insights gained in this assignment to scan social media for covid-related content, and automatically flag it as conspiracy or not conspiracy (for example, for hiding potentially harmful tweets or Facebook posts). Some\n",
        "information about the project and the team:\n",
        "\n",
        "- Your client is a political party concerned about misinformation.\n",
        "- The project requires mining Facebook, Reddit and Instagram data.\n",
        "- The team consists of Joe, an American mathematician who just finished college; Fei, a senior software engineer from China; and Francisco, a data scientist from Spain.\n",
        "\n",
        "Reflect on the impact of exploiting data science for such an application. You should map your discussion to one of the five actions outlined in the UK’s Data Ethics Framework.\n",
        "\n",
        "Your answer should address the following:\n",
        "- Identify the action in which your project is the weakest.\n",
        "- Then, justify your choice by critically analyzing the three key principles for that action outlined\n",
        "in the Framework, namely transparency, accountability and fairness.\n",
        "- Finally, you should propose one solution that explicitly addresses one point related to one of these three principles, reflecting on how your solution would improve the data cycle in this particular use case.\n",
        "\n",
        "Your answer should be between 500 and 700 words. **You are strongly encouraged to follow a scholarly approach, e.g., with references to peer reviewed publications. References do not count towards the word limit**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YJQSO8Amuea"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "Your answer here\n",
        "\n",
        "The principle of transparency requires that data mining processes and their outcomes be made visible, understandable, and accessible to stakeholders, including users, customers, regulators, and other relevant parties. It is particularly important in the context of COVID-related content as the potential for misinformation and manipulation of public opinion is high.\n",
        "\n",
        "The three key principles for the action of transparency are as follows:\n",
        "\n",
        "Accessibility - the data mining process and its outcomes must be easily accessible to stakeholders and users, such as regulators, customers, and other relevant parties. This ensures that users can understand the process and outcomes, and that stakeholders can make informed decisions.\n",
        "\n",
        "Understandability - the data mining process and its outcomes must be presented in a clear and concise manner that is easy to understand. This ensures that users can interpret and make sense of the outcomes and that stakeholders can assess the impact of the process on the wider community.\n",
        "\n",
        "Clarity - the data mining process and its outcomes must be presented in a transparent manner, with no hidden or undisclosed elements. This ensures that the process is open, and that users and stakeholders can make informed decisions and judgments.\n",
        "\n",
        "One possible solution to address the lack of transparency is to develop an online dashboard that presents the data mining process and its outcomes in a clear and accessible manner. The dashboard would provide an overview of the process, including the data sources, algorithms used, and how the data is analyzed. It would also provide an overview of the outcomes, including the number of flagged posts, and the percentage of posts that were flagged as conspiracy theories. This would help to make the process transparent and easily understandable to users and stakeholders.\n",
        "\n",
        "Moreover, it is important to ensure that the system is fair and unbiased, meaning that it is not designed to specifically target a particular group or ideology. This can be achieved by conducting regular audits and reviews of the system to ensure that it is operating in a fair and unbiased manner. Additionally, the team should ensure that the data used in the system is representative of a wide range of sources and viewpoints.\n",
        "\n",
        "In conclusion, while data science can be a powerful tool in identifying and flagging misinformation in social media, it is crucial to ensure that the project adheres to ethical considerations, such as transparency, fairness, and accountability. The development of an online dashboard and conducting regular audits and reviews can help to address the lack of transparency and ensure the system is operating fairly and without bias.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}